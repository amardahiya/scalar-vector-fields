2DO:

 * Follow Python programming standard, especially the use of spaces

======= Python and linear algebra =======

Many people use Matlab as their primary tool for linear algebra. We will here see that all things we can do with Matlab can also be done in a simple way with Python and Numpy.
Python also addresses some things that may seen a bit strange in Matlam: As an example, everything is viewed as a matrix in Matlab, so that if we have a tuple of numbers, is is viewed as a
two-dimensional object. In Python we distinguish more clearly between the two, in that a list can have either one or two dimensions. This prevents Python from misinterpreting a column vector for a row
vector and vice versa, which easily occurs with Matlab.



===== Simple operations =====

If `A` is a matrix, `A.T` is its transpose.
When we want to verify that a result is true, we often generate a matrix at random, and show that the result holds for this matrix. As an example, consider the following code.

!bc pycod
A = matrix(random.rand(4, 4))
B = matrix(random.rand(4, 4))

tol = 1E-14
assert abs((A + B).T - A.T - B.T).max() < tol
assert abs((A * B).T - B.T * A.T).max() < tol
!ec
Which two known results are verified by this code?

The inverse of the matrix `A` can be computed with  `linalg.inv(A)`.

The eigenvectors and eigenvalues of the matrix `A` can be computed by
writing `D,P = linalg.eig(A)`.  `A` must here be a square matrix. The
return value `D` is an array which holds the eigenvalues of `A`, while
the return value `P` is a matrix where the columns are the
(corresponding) eigenvectors of `A` (the eigenvectors are normalized
so that they have length 1, which can make them seem more complicated
than necessary in some cases).

In `numpy` we can compute the scalar product and cross product of two
vectors (arrays) `a` and `b` by writing `dot(a, b)` and `cross(a, b)`,
respectively. The latter assumes that the vectors are
three-dimensional. The (Euclidean) norm of a vector can be computed
with `linalg.norm(a)`. The angle between `a` and `b` can now be
computed by writing

!bc pycod
arccos(dot(a, b)/(linalg.norm(a)*linalg.norm(b)))
!ec

If `c` is an an array or matrix, the sum and product of all its
components can be found by writing `c.sum()` and `c.prod()`. These
functions also take an optional parameters which represents the axis
we should sum over: `c.sum(0)` returns a row vector where each element
is the sum of the corresponding column, and `c.sum(1)` returns a
column vector where each element is the sum of the corresponding row.
`sum()` can be used to compute the mean value of a matrix, and what is
called the Frobenius norm of a matrix, as follows:

!bc pycod
m, n = shape(A)
A.sum()/(m*n)        # The mean value of A
sqrt(sum(A**2))      # The Frobenius norm of A
!ec

In the same way we can compute the maximum value in a matrix `c` by
writing `c.max()`, and along different axes: `c.max(0)` returns a row
vector where each element is the maximum value of the corresponding
column, and `c.max(1)` returns a column vector where each element is
the maximum value of the corresponding row.  The minimum can be
computed similarly.



===== Elementary row operations and Gaussian elimination =====

Let us see how we can bring a matrix to reduced row echelon form.  We
can swap rows `i` and `j` in a matrix `A` by writing `A[[i, j]] =
A[[j, i]`.  We can multiply row `i` with a constant `k` by writing
`A[i] *= k`.  We can add row `i` multiplied with `k` to row `j` by
writing `A[j] += k*A[i]`.

The thress operations can be used to implement Gaussian elimination as follows.

!bc pycod
m, n = shape(A)
for j in range(n - 1):
    for i in range(j + 1, m):
        A[i, j:] -= (A[i,j]/A[j,j])*A[j, j:]
!ec
In this code, we first eliminate the entries below the diagonal in the
first column, by adding a scaled version of the first row to the other
rows, Then the same procedure is applied for the second row, and so
on. The result is an upper triangular matrix. The code can fail if
some of the entries `A[j, j]` become zero along the way. To avoid
this, we can swap rows if this is the case.  The following code does
this, and will not fail even if some of the columns are zero.

!bc pycod
m, n = shape(A)
i = 0
for j in range(n):
    p = argmax(abs(A[i:m, j]))
    if p > 0: # Swap rows
        A[[i, p + i]] = A[[p + i, i]]
    if A[i, j] != 0:
        for r in range(i + 1, m):
            A[r, j:] -= (A[r, j]/A[i, j])*A[i, j:]
        i += 1
    if i>m:
        break
!ec
The rank of a matrix can be computed by first performing Gaussian
elimination, and then count the number of pivot columns in the code
above.  A more reliable way to compute the rank is to compute the
singular value decomposition of `A`, and check how many of the
singular values which are larger than a threshold `epsilon`.

One can ask what other use we can have for performing row operations
manually, besides that an implementation of Gaussian elimination uses
them.  Often one have matrices with a particular structure, where
there for instance are many zeros (also called sparse matrices). When
the location of the non-zeros then are known, it is more efficient to
perform row operations for these locations manually, rather than
perform the full Gaussian elimination.

If `A` is non-singular and quadratic, and `b` is a vector, then the
equation `Ax=b` can be solved by writing `linalg.solve(A,b)`

The components in a matrix can be obtained with simple commands such as

!bc pycod
A[0,1]    # The element in A at row 0, column 1
A[0,:]    # The first row in A
A[:,1]    # The second column in A
!ec
You can also use the colon-notation to pick out other parts of a matrix.
If `C` is a $3\times 5$-matrix,

!bc pycod
C[1:3, 0:4]
!ec
gives a submatrix consisting of the two rows of `C` after the first, and the first four columns of `C`. The command

!bc pycod
C[ ix_([0,2],[1,4])]
!ec
gives a matrix consisting of the first and third rows and second and
fifth column of `A`.  Note the specific function `ix\_` here to
extract a certain set of rows/columns. You can also use this notation
to permute the rows and columns in a matrix. If you write
`C[[2,0,1]]`, the third row is placed first, follows by the first and
second rows. If you write `C[:,[2,0,1,4,3]]`, the third column is
placed first, followed by the first, second, fifth, and fourth,
respectively.

Here are some other commands for producing new matrices:

!bc pycod
A=ones((3,4))      # 3x4-matrix with only ones
A=eye(3)           # 3x3-matrix with ones on the main diagonal,
                   # zeros elsewhere
triu(A)            # Creates an upper triangular matrix from A, i.e.
                   # all elements under the main diagonal is set to 0
tril(A)            # Creates a lower triangular matrix from A, i.e.
                   # all elements over the main diagonal is set to 0
diag(c)            # A diagonal matrix with diagonal elements
                   # given by the vector c.
random.permutation(5) # Creates an arbitrary permutation of the
                      # first five numbers.
!ec

If `A` and `B` are matrices with equally many rows, the matrix where
`A` and `B` are placed next to each other can be computed by writing

!bc pycod
C=hstack([A,B])
!ec
The resulting matrix thus has the same number of rows as `A` and `B`.
Similarly, if `A` and `B` have the same number of columns, the matrix
where `A` and `B` are placed on top of oneanother can be computed by
writing

!bc pycod
C=vstack([A,B])
!ec
The resulting matrix has the same number of columns as `A` and `B`.
While these two functions are specifically for matrices, one also has
the function `concatenate`, which can be applied both for vectors and
matrcies.

The operators +/- computes the componentwise sum/difference of vectors
or matrices.

Multiplication and division have different meanings for `array` and
`matrix` objects. For array objects, mutiplication and division are
performed componentwise, i.e. if `C=A*B`, the components of the
matrices are related by $c_{ij}=a_{ij}*b_{ij}$ (and similarly for
division). This is also called the {\em Hadamard product} of `A` and
`B`. For `matrix`-objects, mutiplication is performed as defined in
linear algebra (section 5.7.5. Include definition). The following is a
straightforward implementation of matrix multiplication

!bc pycod
def mult(A,B):
    m, n = shape(A)
    n1, k =shape(B)
    assert n == n1, 'The dimensions of the matrices do not match!'
    C=zeros((m,k))
    for r in xrange(m):
        for s in xrange(k):
            for t in xrange(n):
                C[r,s] += A[r,t]*B[t,s]
    return C
!ec
If you compare this with running `A*B`, with `A` and `B` being matrix
objects, you will notice a huge difference, in particular when the
matrices are large.  Clearly then, Python does not compute matrix
multiplication with such an implementation.

Since multiplication has a different meaning for arrays and matrices,
we may need to convert between the two. A matrix `A` can be converted
to an array by writin `A = asarray(A)`.

The determinant (include definition, perhaps only in terms of
expansion along rows/columns, and only say some things about its
applications) is another concept which can be implemented in a smarter
way. The following program computes the determinant in a
straightforward way using its definition.

!bc pycod
def detdef(A):
    assert A.shape[0] == A.shape[1], 'The matrix must be quadratic!'
    n = A.shape[0]
    if n == 2: # The determinant of a 2x2-matrix is computed directly
        return A[0,0]*A[1,1] - A[0,1]*A[1,0]
    else: # For larger matrix we expand the determinant along column 0
        determinant = 0.0
        for k in xrange(n): # Create sub-matrix by removing column 0, row k.
            submatrix = vstack((A[0:k,1:n],A[k+1:n,1:n]))
            # Multiply with alternating sign
            determinant += (-1)**k * A[k,0] * detdef(submatrix)
        return determinant
!ec
The `detdef` function is recursive, and calls itself until we have a
matrix where the determinant can be computed directly. [hpl: recursive
functions are not part of INF1100 of the book...]  The central part in
the code is where $(n-1)\times(n-1)$-submatrices are constructed. Note
that in the code we check that the matrices are quadratic.  This code
is not particularly fast either. The determinant can also be computed
with the built-in method `linlag.det(A)`, and this runs much faster,
as the following code verifies.

!bc pycod
from numpy import *
import time

A=random.rand(9,9)
e0=time.time()
linalg.det(A)
print time.time()-e0
e0=time.time()
detdef(A)
print time.time()-e0
!ec
Here an arbitrary $9\times 9$-matrix is constructed, and the
determinant is computed and timed in the two different ways. The
computation times is then written to the display.  Run the code and
see how much faster the built-in determinant function is!  If you
want, also try with an arbitrary $10\times 10$-matrix, but then you
should be patient while the code executes.

===== Symbolic computations =====
In Python one can also perform computations symbolically, using the module `sympy`. This is particularly useful when you have performed tedious calculations by hand, and you want to check if the result is correct. 
Let us go through some examples, covering many typical calculations you do in the first university math courses. 

First note that many of the functions in `numpy` are available with the same names in `sympy`, the difference lying in if numerical or symbolic values are returned. To differ between teh two, we use different names for them
!bc pycode
import numpy as np
import sympy as sp
!ec
Examples of similar functions are 
!bc pyprod
np.sqrt(2), np.cos(np.pi/2), np.exp(2) # numpy
sp.sqrt(2), sp.cos(sp.pi/2), sp.exp(2) #sympy
!ec
Note in particular that `sp.cos(sp.pi/2)`returns the symbolic value for 0, i.e. it does not introduce a roundoff error for $\pi$, and no roundoff error in the computation of the cosine. 
To find the numerical values from the symbolic counterparts we can use the function `evalf()`:
!bc pyprod
sp.sqrt(2).evalf(), sp.exp(2).evalf()
!ec

With `sympy` you can work with rational numbers in an exact way. Rational numbers are represented by objects of type `Rational`. If you write
!bc pycod
expr = sp.Rational(1, 3) + sp.Rational(1, 2)
!ec
the result will be `sp.Rational(5, 6)`. This simple calculation simply says that $5/6 = 1/3 + 1/2$. The corresponding numerical computation can be performed by `expr.evalf()`, yielding $0.83333...$.

These first examples only use constant values. To do more general things we need to use symbols. They can be defined as follows
!bc pycod
x = sp.Symbol('x')            # Define one symbol 
x, y, z = sp.symbols('x y z') # Define three symbols
!ec
Having defined symbols, we can perform symbolic operations such as differentiation and integration. When `x`is defined as a symbol as above, we can differentiate $x^5$ with respect to $x$ by writing
!bc pycod
expr = sp.diff(x**5, x)
!ec
This yields the symbolic expression for $5x^4$. If we now want to evaluate the derivative at $x=2$ we can write 
!bc pycod
expr.subs(x, 2)
!ec
If we want the numerical value of the derivative we can again use `evalf()` on this expression. 

For integration, the indefinite integral $\int x^7e^{x^2}dx$ can be computed by writing
!bc pycod
sp.integrate(x**7*sp.exp(x**2),x)
!ec
This yields the expression for $(x^6 - 3x^4 + 6x^2 - 6)e^{x^2}$. In order to obtain the same result by hand, you would need to perform integration by parts three times in succession. 
To compute the definite integral $\int_0^1 x^7e^{x^2}dx$, one can simply write 
!bc pycod
sp.integrate(x**7*sp.exp(x**2), (x, 0, 1))
!ec
This yields the expression for $-e+3$.

It is also simple to multiply or factor polynomials with the function `expand` and `factor`. The calls
!bc pycod
sp.expand((x+3)**3, x)
sp.factor(x**2-4*x+4, x) 
!ec
yields the expressions for $x^3 + 9x^2 + 27x + 27$, and $(x-2)^2$, respectively. 

To define matrices we can write
!bc pycod
A = sp.Matrix([[1, 2, 0],[0, 1, 1],[0, -2, 1]])
!ec
To find the eigenvalues and eigenvectors of $A$ exactly, we can write
!bc pycod
A.eigenvals()
A.eigenvects()
!ec
You will see that the exact expressions are found. You are invited to verify these results by hand, although this is a rather tedious calculation. 
The inverse matrix of $A$ can be found by writing `A**-1`. Also here `sympy` is able to find the exact mathematical expression. Since the inverse can be computed from elementary row operations, 
and all these can be carried out exactly when the entries have exact representations, `sympy` can find the inverse exactly for any matrix size, given that the entries are rational numbers. 
The same is the case for matrix multiplication.

Elementary row operations can be carried out exactly in `sympy`. If you write 
!bc pycod
A[2,:] = A[2,:] + 2 * A[1,:]
!ec
for the matrix `A` as defined above, the entry `A[2, 1]` will be zeroed out, so that the resulting matrix is in row echelon form. 

We can also use `sympy`to solve linear systems exactly. If you write
!bc pycod
A.solve(sp.Matrix([[-1],[-1],[-1]]))
!ec
The system $Ax=b$ will be solved, when the right hand side $b$ have all entries equal to $-1$.




===== Plotting in three dimensions with scitools =====

There is also support for drawing vector fields. If you write

!bc pycod
quiver(x,y,u,v)
!ec
a vector field will be drawn where the `x`- og `y`-vectors specify
point where vectors are to be drawn, and where the `u`- and
`v`-vectors specify the vectors to be drawn at these points. This
means that at every point $(x,y)$, the vector $(u,v)$ is drawn.

If you drop the $x$- og $y$ parameters, the vectors will be drawn in a
standard grid where all points have integer coordinates larger than
zero.

When we plot a vector field it can be smart not to use too many
points, since one risks that some vectors are drawn that the collide
with oneanother, rendering a very cluttered plot.
