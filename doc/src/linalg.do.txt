========= Linear algebra =========

Python has strong support for numerical linear algebra, much like the
functionality found in Matlab.
#Contrary to Matlab, where the fundamental data structure is the matrix,
#the NumPy package in Python features arrays of any dimension.



======= Some common linear algebra operations =======

===== Inverse, determinant, and eigenvalues =====
idx{eigenvalues} idx{eigenvectors}
idx{inverse} idx{determinant}

We start with exemplifying how to find the inverse and the determinant of a
matrix, and how to compute the eigenvalues and eigenvectors:

!bc pyshell
>>> import numpy as np
>>> A = np.array([[2, 0], [0, 5]], dtype=float)

>>> np.linalg.inv(A)  # inverse matrix
array([[ 0.5,  0. ],
       [ 0. ,  0.2]])

>>> np.linalg.det(A)  # determinant
9.9999999999999982

>>> eig_values, eig_vectors = np.linalg.eig(A)
>>> eig_values
array([ 2.,  5.])
>>> eig_vectors
array([[ 1.,  0.],
       [ 0.,  1.]])
!ec
The eigenvectors are normalized to have unit lengths.


===== Products =====

idx{`dot` (from `numpy`)}
idx{`cross` (from `numpy`)}

The `np.dot` function is used for scalar or dot product as well as
matrix-vector and matrix-matrix products:

!bc pyshell
>>> a = np.array([4, 0])
>>> b = np.array([0, 1])
>>> np.dot(A, a)         # matrix vector product
array([ 8.,  0.])
>>> np.dot(a, b)         # dot product between vectors
0
>>>
>>> B = np.ones((2, 2))  # 2x2 matrix with 1's
>>> np.dot(A, B)         # matrix-matrix product
array([[ 2.,  2.],
       [ 5.,  5.]])
!ec

The cross product $a\times b$,
between vectors $a$ and $b$ of length 3, is computed by

!bc pyshell
>>> np.cross([1, 1, 1], [0, 0, 1])
array([ 1, -1,  0])
!ec
Finding the angle between vectors,

!bt
\[ \theta = \cos^{-1}\left(\frac{a\cdot b}{||a||\,||b||}\right),\]
!et
go like

!bc pyshell
>>> np.arccos(np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b)))
1.5707963267948966
!ec

===== Norms =====
idx{norms in `numpy`}

Various norms of matrices and vectors are well supported by NumPy.
Some common examples are

!bc pyshell
>>> np.linalg.norm(A)        # Frobenius norm for matrices
5.3851648071345037
>>> np.sqrt(np.sum(A**2))    # Frobenius norm, direct formula
5.3851648071345037
>>> np.linalg.norm(a)        # l2 norm for vectors
4.0
!ec
See `pydoc numpy.linalg.norm` for information on other norms.


===== Sum and extreme values =====

idx{`sum` (from `numpy`)}
idx{`max` (from `numpy`)}

The sum of all elements or of the elements in a particular row or column
is computed by `np.sum`:

!bc pyshell
>>> np.sum(B)          # sum of all elements
2.0
>>> np.sum(B, axis=0)  # sum over index 0 (rows)
array([ 4., -2.])
>>> np.sum(B, axis=1)  # sum over index 1 (columns)
array([ 3., -1.])
!ec
The maximum or minimum value of an array is also often needed:

!bc pyshell
>>> np.max(B)          # max over all elements
3.0
>>> B.max()            # max over all elements, alternative syntax
3.0
>>> np.min(B)          # min over all elements
-4.0
>>> B.min()            # min over all elements, alternative syntax
-4.0
>>> np.abs(B).min()    # min absolute value
1.0
!ec
A very frequent application of computing the minimum absolute value
occurs in test functions where we want to verify a result, e.g.,
that $AA^{-1}=I$, where $I$ is the identity matrix:

!bc pyshell
>>> I = np.eye(2)   # identity matrix of size 2
>>> I
array([[ 1.,  0.],
       [ 0.,  1.]])
>>> np.abs(np.dot(A, np.linalg.inv(A)) - I).max()
0.0
!ec

!bwarning Never use `==` when testing real numbers!
It could be tempting to test $AA^{-1}=I$ using the syntax

!bc pyshell
>>> np.dot(A, np.linalg.inv(A)) == np.eye(2)
array([[ True,  True],
       [ True,  True]], dtype=bool)
!ec
but there are two major problems with this test:

 o the result is a boolean matrix, not suitable for an `if` test
 o using `==` for matrices with float elements may fail because of
   rounding errors

The second problem must be solved by
computing differences and comparing them against small tolerances, as we
did above.
Here is an example where `==` fails:

!bc pyshell
>>> A = np.array([[4, 0], [0, 49]], dtype=float)
>>> np.dot(A, np.linalg.inv(A)) == np.eye(2)
array([[ True,  True],
       [ True, False]], dtype=bool)
!ec
(`1.0/49*49` is not exactly `1` because of rounding errors.)

The first problem is solved by using the `C.all()`, which returns one
boolean variable `True` if all elements in the boolean array `C` are `True`,
otherwise it returns `False`, as in the case above:

!bc pyshell
>>> (np.dot(A, np.linalg.inv(A)) == np.eye(2)).all()
False
!ec
!ewarning

===== Indexing =====

Indexing an element is done by `A[i,j]`, and a row or column
is extracted as

!bc pyshell
>>> A[0,:]  # first row
array([ 2.,  0.])
>>> A[:,1]  # second column
array([ 0.,  5.])
!ec
NumPy also supports
multiple values for the indices via the `np.ix_`
function. Here is an example where we
grab row 0 and 2, then column 1:

!bc pyshell
>>> C = np.array([[1,2,3],[4,5,6],[7,8,9]])
>>> C[np.ix_([0,2], [1])]  # row 0 and 2, then column 1
array([[2],
       [8]])
!ec

You can also use the colon notation to pick out other parts of a matrix.
If `C` is a $3\times 5$-matrix,

!bc pycod
C[1:3, 0:4]
!ec
gives a sub-matrix consisting of the two rows of `C` after the first, and the first four columns of `C` (recall that the upper limits, here `3` and `4`, are
not included).

If you are familiar with Matlab you should note that behavior may be a
bit unexpected when referring to parts of a matrix. Writing `C[[0, 2],
[0, 2]]` one would expect entries residing in rows/columns $0$ and
$2$. The following code shows that in Python a special command with
`np.ix_` is required for this:

!bc pyshell
>>> C = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
>>> C[[0, 2], [0, 2]]
[1 9]
>>> C[np.ix_([0, 2], [0, 2])]
[[1 3]
 [7 9]]
!ec

===== Other frequent operations =====

idx{transpose (in `numpy`)}
The transpose of a matrix `B` is obtained by `B.T`:

!bc pyshell
>>> B = np.array([[1, 2], [3, -4]], dtype=float)
>>> B.T                # the transpose
array([[ 1.,  3.],
       [ 2., -4.]])
!ec

NumPy can also strip down a matrix to its upper or lower triangular parts:

!bc pyshell
>>> np.triu(B)  # upper triangular part of B
array([[ 1.,  2.],
       [ 0., -4.]])
>>> np.tril(B)  # lower triangular part of B
array([[ 1.,  0.],
       [ 3., -4.]])
!ec

===== Solving linear systems =====

The perhaps most frequent operation in linear algebra is the solution
of systems of linear algebraic equations: $Ax=b$, where $A$ is a coefficient
matrix, $b$ is a given right-hand side vector, and $x$ is the solution vector.
The function `np.linalg.solve(A, b)` does the job:

!bc pyshell
>>> A = np.array([[1, 2], [-2, 2.5]])
>>> x = np.array([-1, 1], dtype=float)  # solution
>>> b = np.dot(A, x)                    # right-hand side

>>> np.linalg.solve(A, b)               # will it compute x?
array([-1.,  1.])
!ec

===== Matrix row and column operations =====

Implementing Gaussian elimination constitutes a good pedagogical example on
various row and column operations on a matrix. Some
needed functionality is

!bc pycod
A[[i, j]] = A[[j, i]]  # swap rows i and j
A[i] *= k              # multiply row i by a constant k
A[j] += k*A[i]         # add row i, multiplied by k, to row j
!ec
With these operations, Gaussian elimination is programmed as follows.


!bc pycod
m, n = shape(A)
for j in range(n - 1):
    for i in range(j + 1, m):
        A[i, j:] -= (A[i,j]/A[j,j])*A[j, j:]
!ec
Note the special syntax `j:`, which refers to indices from j and up to the end of the array.
More generally, when referring to an array `a` with length `n`, the following are equivalent:
!bc pycod
a[0:n]
a[:n]
a[0:]
a[:]
!ec
In the code for Gaussian elimination, we first eliminate the entries below the diagonal in the
first column, by adding a scaled version of the first row to the other
rows, Then the same procedure is applied for the second row, and so
on. The result is an upper triangular matrix. The code can fail if
some of the entries `A[j, j]` become zero along the way. To avoid
this, we can swap rows if the problem arises.  The following code
implements the idea and will not fail, even if some of the columns are zero.

!bc pycod
def Gaussian_elimination(A):
    rank = 0
    m, n = np.shape(A)
    i = 0
    for j in range(n):
        p = np.argmax(abs(A[i:m,j]))
        if p > 0: # swap rows
            A[[i,p+i]] = A[[p+i, i]]
        if A[i,j] != 0:
            # j is a pivot column
            rank += 1
            for r in range(i+1, m):
                A[r,j:] -= (A[r,j]/A[i,j])*A[i,j:]
            i += 1
        if i > m:
            break
    return rank
!ec

===== Computing the rank of a matrix =====

idx{rank of a matrix}

The rank of a matrix equals the number of pivot columns after Gaussian elimination.
The variable `rank` counts these in the code above.

Due to rounding errors, the computed rank may be higher than the
actual rank: The rounding errors may imply that `A[i,j] != 0` is true,
even if Gaussian elimination performed exactly gives exactly zero.
These kinds of situations can be avoided by replacing `if A[i,j] !=0:`
with `if abs(A[i, j]) > tol:`, where `tol` is some small
tolerance.

A more reliable way to compute the rank is to compute the
singular value decomposition of `A`, and check how many of the
singular values which are larger than a threshold `epsilon`:

!bc pyshell
>>> A = np.array([[1, 2.01], [2.01, 4.0401]])
>>> U, s, V = np.linalg.svd(A) # s are the singular values of A
>>> shape((abs(s) > tol).nonzero())[1]
1
# abs(s) > tol gives an array with True and False values
# s.nonzero() lists indices k so that s[k] != 0
>>>  Gaussian_elimination(A)
2
!ec
If you use a tolerance check on the form `if abs(A[i,j]) > 1E-10:`
in the function `Gaussian_elimination`, the code will say that the rank is 1,
which is the correct value also found by using the singular value
decomposition. It seems that `np.linalg.svd(A)` also rounds small
singular values to zero, so that one can find the rank simply as
`shape(s.nonzero())[1]` above.

It is known that the determinant is nonzero if and only if the rank equals the number of rows/columns.
For the matrix $A$ we used above, the determinant should thus be $0$,
but also here roundoff errors come into play:

!bc pyshell
>>> A = np.array([[1, 2.01], [2.01, 4.0401]])
>>> A[0, 0]*A[1, 1] - A[0, 1]*A[1, 0]
8.881784197e-16
>>> np.linalg.det(A)
8.92619311799e-16
!ec

Using Gaussian elimination for computing the rank may not be very
efficient. The following code compares the computational time for a
random $100\times 100$-matrix to that of using a singular value
decomposition.

!bc pycod
>>> A = np.random.rand(100, 100)
>>> %timeit U, s, V = np.linalg.svd(A)
100 loops, best of 3: 3.7 ms per loop
>>> %timeit Gauss_elimination(A)
100 loops, best of 3: 22.3 ms per loop
!ec






===== Symbolic linear algebra =====

SymPy supports symbolic computations also for linear algebra operations.
We may create a matrix and find its inverse and determinant:

!bc pyshell
>>> import sympy as sym
>>> A = sym.Matrix([[2, 0], [0, 5]])

>>> A**-1    # the inverse
Matrix([
[1/2,   0],
[  0, 1/5]])

>>> A.inv()  # the inverse
Matrix([
[1/2,   0],
[  0, 1/5]])

>>> A.det()  # the determinant
10
!ec
Note that the entries in the inverse matrix are rational numbers
(`sym.Rational` objects to be precise).

Eigenvalues can also be computed exactly:

!bc pyshell
>>> A.eigenvals()
{2: 1, 5: 1}
!ec
The output is a dictionary (see Chapter 6) [hpl: need exact reference here]
meaning here that 2 is an eigenvalue with multiplicity 1 and 5 is an
eigenvalue with multiplicity 1. We can collect them in a list by

!bc pyshell
>>> e = list(A.eigenvals().keys())
>>> e
[2, 5]
!ec
Eigenvector computations has a somewhat complicated output:

!bc pyshell
>>> A.eigenvects()
[(2, 1, [Matrix([
[1],
[0]])]), (5, 1, [Matrix([
[0],
[1]])])]
!ec
The output is a list of three-tuples, one for each eigenvalue and eigenvector.
The three-tuple contains the eigenvalue, its multiplicity, and the eigenvector
as a `sym.Matrix` object.
To isolate the first eigenvector, we can index the list and tuple:

!bc pyshell
>>> v1 = A.eigenvects()[0][2]
>>> v1
Matrix([
[1],
[0]])
!ec
The vector is a `sym.Matrix` object with two indices. To extract the
vector elements in a plain list, we can do this:

!bc pyshell
>>> v1 = [v1[i,0] for i in range(v1.shape[0])]
>>> v1
[1, 0]
!ec
The following code extracts all eigenvectors as a list of 2-lists:

!bc pyshell
>>> v = [[t[2][0][i,0] for i in range(t[2][0].shape[0])]
         for t in A.eigenvects()]
>>> v
[[1, 0], [0, 1]]
!ec

The norm of a matrix or vector is an exact expression:

!bc pyshell
>>> A.norm()
sqrt(29)
>>> a = sym.Matrix([1, 2])
>>> a
Matrix([
[1],
[2]])
>>> a.norm()
sqrt(5)
!ec

The matrix-vector product and the dot product between vectors are
done like this:

!bc pyshell
>>> A*a
Matrix([
[ 2],
[10]])
>>> b = sym.Matrix([2, -1])
>>> a.dot(b)
0
!ec

Solving linear systems exactly is also possible:

!bc pyshell
>>> x = sym.Matrix([-1, 1])/2
>>> x
Matrix([
[-1/2],
[ 1/2]])
>>> b = A*x
>>> x = A.LUsolve(b)  # does it compute x?
>>> x                 # x is a matrix object
Matrix([
[-1/2],
[ 1/2]])
!ec
Sometimes one wants to convert `x` to a plain `numpy` array with
`float` values:

!bc pyshell
>>> x = np.array([float(x[i,0].evalf()) for i in range(x.shape[0])])
>>> x
array([-0.5,  0.5])
!ec

Exact row operations can be done as exemplified here:

!bc pyshell
>>> A[1,:] + 2*A[0,:]  # [0,5] + 2*[2,0]
Matrix([[4, 5]])
!ec
We refer to the online "SymPy linear algebra tutorial": "http://docs.sympy.org/dev/tutorial/matrices.html" for more information.

======= Exercises =======

===== Exercise: Verify linear algebra results =====
label{plot:linalg:exer:verify}
file=verify_linalg

When we want to verify that a mathematical result is true, we often generate matrices or vectors with random elements and show that the result holds for
these ``arbitrary'' mathematical objects. As an example,
consider testing that $A+B=B+A$ for matrices $A$ and $B$:

!bc pycod
def test_addition():
    n = 4  # matrix size
    A = matrix(random.rand(n, n))
    B = matrix(random.rand(n, n))

    tol = 1E-14
    result1 = A + B
    result2 = B + A
    assert abs(result1 - result2).max() < tol
!ec
Use this technique to write test functions for the following mathematical
results:

o $(A + B)C = AC + BC$
o $(AB)C = A(BC)$
o $\text{rank}A=\text{rank}A^T$
o $\det(AB) = \det A\det B$
o The eigenvalues if $A$ equals the eigenvalues of $A^T$ when $A$ is square.

