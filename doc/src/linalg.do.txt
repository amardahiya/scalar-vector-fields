========= Linear algebra =========

Python has strong suppoert for numerical linear algebra, much like the
functionality found in Matlab.
#Contrary to Matlab, where the fundamental data structure is the matrix,
#the NumPy package in Python features arrays of any dimension.



======= Basic operations =======

idx{eigenvalues} idx{eigenvectors}

First we exemplify how to find
the inverse and the determinant of a matrix, and how to
compute the eigenvalues and eigenvectors:

!bc pyshell
>>> import numpy as np
>>> A = np.array([[2, 0], [0, 5]], dtype=float)

>>> np.linalg.inv(A)  # inverse matrix
array([[ 0.5,  0. ],
       [ 0. ,  0.2]])

>>> np.linalg.det(A)  # determinant
9.9999999999999982

>>> eig_values, eig_vectors = np.linalg.eig(A)
>>> eig_values
array([ 2.,  5.])
>>> eig_vectors
array([[ 1.,  0.],
       [ 0.,  1.]])
!ec
The eigenvectors are normalized
so that they have length 1, which can make them seem more complicated
than necessary in some cases.

idx{`dot` (from `numpy`)}
idx{`cross` (from `numpy`)}

The `np.dot` function is used for scalar or dot product as well as
matrix-vector and matrix-matrix products:

!bc pyshell
>>> a = np.array([4, 0])
>>> b = np.array([0, 1])
>>> np.dot(A, a)         # matrix vector product
array([ 8.,  0.])
>>> np.dot(a, b)         # dot product between vectors
0
>>>
>>> B = np.ones((2, 2))  # 2x2 matrix with 1's
>>> np.dot(A, B)         # matrix-matrix product
array([[ 2.,  2.],
       [ 5.,  5.]])
!ec

Other typical operations in linear algebra, like the cross product $a\times b$
(between vectors of length 3) and finding the angle between vectors,

!bt
\[ \theta = \cos^{-1}\left(\frac{a\cdot b}{||a||\,||b||}\right),\]
!et
go like

!bc pyshell
>>> np.cross([1, 1, 1], [0, 0, 1])
array([ 1, -1,  0])

>>> np.arccos(np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b)))
1.5707963267948966
!ec

idx{norms in `numpy`}

Various norms of matrices and vectors are well supported by NumPy.
Some common examples are

!bc pyshell
>>> np.linalg.norm(A)        # Frobenius norm for matrices
5.3851648071345037
>>> np.sqrt(np.sum(A**2))    # Frobenius norm, direct formula
5.3851648071345037
>>> np.linalg.norm(a)        # l2 norm for vectors
4.0
!ec
See `pydoc numpy.linalg.norm` for information on other norms.

idx{transpose (in `numpy`)}
The transpose of a matrix `B` is obtained by `B.T`:

!bc pyshell
>>> B = np.array([[1, 2], [3, -4]], dtype=float)
>>> B.T                # the transpose
array([[ 1.,  3.],
       [ 2., -4.]])
!ec

idx{`sum` (from `numpy`)}
idx{`max` (from `numpy`)}

The sum of all elements or of the elements in a particular row or column
is computed by `np.sum`:

!bc pyshell
>>> np.sum(B)          # sum of all elements
2.0
>>> np.sum(B, axis=0)  # sum over index 0 (rows)
array([ 4., -2.])
>>> np.sum(B, axis=1)  # sum over index 1 (columns)
array([ 3., -1.])
!ec
The maximum or minimum value of an array is also often needed:

!bc pyshell
>>> np.max(B)          # max over all elements
3.0
>>> B.max()            # max over all elements, alternative syntax
3.0
>>> np.min(B)          # min over all elements
-4.0
>>> B.min()            # min over all elements, alternative syntax
-4.0
>>> np.abs(B).min()    # min absolute value
1.0
!ec
A very frequent application of computing the minimum absolute value
occurs in test functions where we want to verify a result, e.g.,
that $AA^{-1}=I$, where $I$ is the identity matrix:

!bc pyshell
>>> I = np.eye(2)   # identity matrix of size 2
>>> I
array([[ 1.,  0.],
       [ 0.,  1.]])
>>> np.abs(np.dot(A, np.linalg.inv(A)) - I).max()
0.0
!ec
It could be tempting to test $AA^{-1}=I$ using the syntax

!bc pyshell
>>> np.dot(A, np.linalg.inv(A)) == np.eye(2)
array([[ True,  True],
       [ True,  True]], dtype=bool)
!ec
but there are two major problems with this test:

 o the result is a boolean matrix, not suitable for an `if` test
 o using `==` for matrices with float elements may fail because of
   rounding errors

The second problem must be solved by
computing differences and comparing them against small tolerances, as we
did above.
Here is an example where `==` fails:

!bc pyshell
>>> A = np.array([[4, 0], [0, 49]], dtype=float)
>>> np.dot(A, np.linalg.inv(A)) == np.eye(2)
array([[ True,  True],
       [ True, False]], dtype=bool)
!ec
(`1.0/49*49` is not exactly `1` because of rounding errors.)

The first problem is solved by using the `C.all()`, which returns one
boolean variable `True` if all elements in the boolean array `C` are `True`,
otherwise it returns `False`, as in the case above:

!bc pyshell
>>> (np.dot(A, np.linalg.inv(A)) == np.eye(2)).all()
False
!ec

NumPy can also strip down a matrix to its upper or lower triangular parts:

!bc pyshell
>>> np.triu(B)  # upper triangular part of B
array([[ 1.,  2.],
       [ 0., -4.]])
>>> np.tril(B)  # lower triangular part of B
array([[ 1.,  0.],
       [ 3., -4.]])
!ec

Indexing an element is done by `B[i,j]`, and a row or column
is extracted as

!bc pyshell
>>> A[0,:]  # first row
array([ 2.,  0.])
>>> A[:,1]  # second column
array([ 0.,  5.])
!ec
NumPy also supports
multiple values for the indices via the `np.ix_`
function. Here is an example where we
grab row 0 and 2, then column 1:

!bc pyshell
>>> C = np.array([[1,2,3],[4,5,6],[7,8,9]])
>>> C[np.ix_([0,2], [1])]  # row 0 and 2, then column 1
array([[2],
       [8]])
!ec

You can also use the colon-notation to pick out other parts of a matrix.
If `C` is a $3\times 5$-matrix,

!bc pycod
C[1:3, 0:4]
!ec
gives a submatrix consisting of the two rows of `C` after the first, and the first four columns of `C` (recall that the upper limits, here `3` and `4`, are
not included).



======= Row operations and Gaussian elimination =======

The perhaps most frequent operation in linear algebra is the solution
of systems of linear algebraic equations: $Ax=b$, where $A$ is a coefficient
matrix, $b$ is a given right-hand side vector, and $x$ is the solution vector.
The function `np.linalg.solve(A, b)` does the job:

!bc pyshell
>>> A = np.array([[1, 2], [-2, 2.5]])
>>> x = np.array([-1, 1], dtype=float)  # solution
>>> b = np.dot(A, x)                    # right-hand side

>>> np.linalg.solve(A, b)               # will it compute x?
array([-1.,  1.])
!ec

Implementing Gaussian elimination constitutes a good example on
various row and column operations on a matrix. Some
needed functionality is

!bc pycod
A[[i, j]] = A[[j, i]   # swap rows i and j
A[i] *= k              # multiply row i by a constant k
A[j] += k*A[i]         # add row i, multiplied by k, to row j
!ec
With these operations, Gaussian elimination is programmed as follows.

[hpl: Here a new syntax `j:`. Must be explained.]

!bc pycod
m, n = shape(A)
for j in range(n - 1):
    for i in range(j + 1, m):
        A[i, j:] -= (A[i,j]/A[j,j])*A[j, j:]
!ec
In this code, we first eliminate the entries below the diagonal in the
first column, by adding a scaled version of the first row to the other
rows, Then the same procedure is applied for the second row, and so
on. The result is an upper triangular matrix. The code can fail if
some of the entries `A[j, j]` become zero along the way. To avoid
this, we can swap rows if the problem arises.  The following code
implements the idea and will not fail, even if some of the columns are zero.

!bc pycod
m, n = shape(A)
i = 0
for j in range(n):
    p = argmax(abs(A[i:m, j]))
    if p > 0: # swap rows
        A[[i, p + i]] = A[[p + i, i]]
    if A[i, j] != 0:
        for r in range(i + 1, m):
            A[r, j:] -= (A[r, j]/A[i, j])*A[i, j:]
        i += 1
    if i>m:
        break
!ec

idx{rank of a matrix}

The rank of a matrix can be computed by first performing Gaussian
elimination, and then count the number of pivot columns in the code
above:

!bc pycod
PROVIDE EXAMPLE!
!ec
A more reliable way to compute the rank is to compute the
singular value decomposition of `A`, and check how many of the
singular values which are larger than a threshold `epsilon`:

!bc pycod
PROVIDE EXAMPLE!
!ec

# [hpl: I don't think this makes sense without a specific example,
# and that's beyond the scope of the book.]
#One can ask what other use we can have for performing row operations
#manually, besides that an implementation of Gaussian elimination uses
#them.  Often one have matrices with a particular structure, where
#there for instance are many zeros (also called sparse matrices). When
#the location of the non-zeros then are known, it is more efficient to
#perform row operations for these locations manually, rather than
#perform the full Gaussian elimination.

[hpl: Didn't see where this info fits well - should have some
application since the usefullness is not obvious to readers...]

You can use the `np.ix_` notation
to permute the rows and columns in a matrix. If you write
`C[[2,0,1]]`, the third row is placed first, follows by the first and
second rows. If you write `C[:,[2,0,1,4,3]]`, the third column is
placed first, followed by the first, second, fifth, and fourth,
respectively.


If `A` and `B` are matrices with equally many rows, the matrix where
`A` and `B` are placed next to each other or on top of each other
can be computed by writing

!bc pyshell
>>> np.hstack([A, B]) # stack two arrays horizontally
array([[ 1. ,  2. ,  1. ,  2. ],
       [-2. ,  2.5,  3. , -4. ]])
>>> np.vstack([A, B]) # stack two arrays vertically
array([[ 1. ,  2. ],
       [-2. ,  2.5],
       [ 1. ,  2. ],
       [ 3. , -4. ]])
>>> np.vstack([a, b]) # stack two vectors vertically
array([[ 4. ,  0. ],
       [ 1. ,  4.5]])
!ec
The related `np.concatenate` function is used to ``append'' arrays
to each other:

!bc pyshell
>>> np.concatenate([A, B])
array([[ 1. ,  2. ],
       [-2. ,  2.5],
       [ 1. ,  2. ],
       [ 3. , -4. ]])
>>> np.concatenate([a, b])
array([ 4. ,  0. ,  1. ,  4.5])
!ec

[hpl: I deleted material on matrix objects and the multiplication
operator as there is already a subsection on this topic in the book.]

The determinant (include definition, perhaps only in terms of
expansion along rows/columns, and only say some things about its
applications) is another concept which can be implemented in a smarter
way. The following program computes the determinant in a
straightforward way using its definition.

[hpl: This function is recursive - too complicated for the book,
I think. We do recursive stuff in chapter 9, though. I suggest we
leave this function out.]

!bc pycod
def detdef(A):
    assert A.shape[0] == A.shape[1], 'The matrix must be quadratic!'
    n = A.shape[0]
    if n == 2: # The determinant of a 2x2-matrix is computed directly
        return A[0,0]*A[1,1] - A[0,1]*A[1,0]
    else: # For larger matrix we expand the determinant along column 0
        determinant = 0.0
        for k in xrange(n): # Create sub-matrix by removing column 0, row k.
            submatrix = vstack((A[0:k,1:n],A[k+1:n,1:n]))
            # Multiply with alternating sign
            determinant += (-1)**k * A[k,0] * detdef(submatrix)
        return determinant
!ec
The `detdef` function is recursive, and calls itself until we have a
matrix where the determinant can be computed directly. The central part in
the code is where $(n-1)\times(n-1)$-submatrices are constructed. Note
that in the code we check that the matrices are quadratic.  This code
is not particularly fast either. The determinant can also be computed
with the built-in method `linlag.det(A)`, and this runs much faster,
as the following code verifies.

!bc pycod
from numpy import *
import time

A=random.rand(9,9)
e0=time.time()
linalg.det(A)
print time.time()-e0
e0=time.time()
detdef(A)
print time.time()-e0
!ec
Here an arbitrary $9\times 9$-matrix is constructed, and the
determinant is computed and timed in the two different ways. The
computation times is then written to the display.  Run the code and
see how much faster the built-in determinant function is!  If you
want, also try with an arbitrary $10\times 10$-matrix, but then you
should be patient while the code executes.

[hpl: I think it's better to use `timeit` in IPython for such
CPU time tests, but we leave the determinant code out.]

===== Symbolic linear algebra =====

[hpl: I have a short intro to SymPy in chapter 1 already, much like what
you wrote in the SymPy file, so here I think we limit the attention to
linear algebra with SymPy.]

SymPy supports symbolic computations also for linear algebra operations.
We may create a matrix and find its inverse and determinant:

!bc pyshell
>>> import sympy as sym
>>> A = sym.Matrix([[2, 0], [0, 5]])

>>> A**-1    # the inverse
Matrix([
[1/2,   0],
[  0, 1/5]])

>>> A.inv()  # the inverse
Matrix([
[1/2,   0],
[  0, 1/5]])

>>> A.det()  # the determinant
10
!ec
Note that the entries in the inverse matrix are rational numbers
(`sym.Rational` objects to be precise).

Eigenvalues can also be computed exactly:

!bc pyshell
>>> A.eigenvals()
{2: 1, 5: 1}
!ec
The output is a dictionary (see Chapter 6) [hpl: need exact reference here]
meaning here that 2 is an eigenvalue with multiplicity 1 and 5 is an
eigenvalue with multiplicity 1. We can collect them in a list by

!bc pyshell
>>> e = list(A.eigenvals().keys())
>>> e
[2, 5]
!ec
Eigenvector computations has a somewhat complicated output:

!bc pyshell
>>> A.eigenvects()
[(2, 1, [Matrix([
[1],
[0]])]), (5, 1, [Matrix([
[0],
[1]])])]
!ec
The output is a list of three-tuples, one for each eigenvalue and eigenvector.
The three-tuple contains the eigenvalue, its multiplicity, and the eigenvector
as a `sym.Matrix` object.
To isolate the first eigenvector, we can index the list and tuple:

!bc pyshell
>>> v1 = A.eigenvects()[0][2]
>>> v1
Matrix([
[1],
[0]])
!ec
The vector is a `sym.Matrix` object with two indices. To extract the
vector elements in a plain list, we can do this:

!bc pyshell
>>> v1 = [v1[i,0] for i in range(v1.shape[0])]
>>> v1
[1, 0]
!ec
The following code extracts all eigenvectors as a list of 2-lists:

!bc pyshell
>>> v = [[t[2][0][i,0] for i in range(t[2][0].shape[0])]
         for t in A.eigenvects()]
>>> v
[[1, 0], [0, 1]]
!ec

The norm of a matrix or vector is an exact expression:

!bc pyshell
>>> A.norm()
sqrt(29)
>>> a = sym.Matrix([1, 2])
>>> a
Matrix([
[1],
[2]])
>>> a.norm()
sqrt(5)
!ec

The matrix-vector product and the dot product between vectors are
done like this:

!bc pyshell
>>> A*a
Matrix([
[ 2],
[10]])
>>> b = sym.Matrix([2, -1])
>>> a.dot(b)
0
!ec

Solving linear systems exactly is also possible:

!bc pyshell
>>> x = sym.Matrix([-1, 1])/2
>>> x
Matrix([
[-1/2],
[ 1/2]])
>>> b = A*x
>>> x = A.LUsolve(b)  # does it compute x?
>>> x                 # x is a matrix object
Matrix([
[-1/2],
[ 1/2]])
!ec
Sometimes one wants to convert `x` to a plain `numpy` array with
`float` values:

!bc pyshell
>>> x = np.array([float(x[i,0].evalf()) for i in range(x.shape[0])])
>>> x
array([-0.5,  0.5])
!ec

Exact row operations can be done as exemplified here:

!bc pyshell
>>> A[1,:] + 2*A[0,:]  # [0,5] + 2*[2,0]
Matrix([[4, 5]])
!ec
We refer to the online "SymPy linear algebra tutorial": "http://docs.sympy.org/dev/tutorial/matrices.html" for more information.

======= Exercises =======

===== Exercise: Verify linear algebra results =====
label{plot:linalg:exer:verify}
file=verify_linalg

When we want to verify that a mathematical result is true, we often generate matritces or vectors with random elements and show that the result holds for
these ``arbitrary'' mathematical objects. As an example,
consider testing that $A+B=B+A$ for matrices $A$ and $B$:

!bc pycod
def test_addition():
    n = 4  # matrix size
    A = matrix(random.rand(n, n))
    B = matrix(random.rand(n, n))

    tol = 1E-14
    result1 = A + B
    result2 = B + A
    assert abs(result1 - result2).max() < tol
!ec
Use this technique to write test functions for the following mathematical
results:

[hpl: List a set of linear algebra results here to be verified computationally!]

